from typing import TypedDict, Annotated
from langgraph.graph import add_messages, StateGraph, END
from langchain_groq import ChatGroq
from langchain_core.messages import AIMessage, HumanMessage
from dotenv import load_dotenv
from langgraph.checkpoint.memory import MemorySaver, InMemorySaver, PersistentDict

load_dotenv()

memory = MemorySaver()

llm = ChatGroq(model="llama-3.1-8b-instant")

class BasicChatState(TypedDict): 
    messages: Annotated[list, add_messages]

def chatbot(state: BasicChatState): 
    return {
       "messages": [llm.invoke(state["messages"])]
    }

graph = StateGraph(BasicChatState)

graph.add_node("chatbot", chatbot)

graph.add_edge("chatbot", END)

graph.set_entry_point("chatbot")

app = graph.compile(checkpointer=memory)

config = {"configurable": {
    "thread_id": 1
}}

"""
The `thread_id` in the configuration serves as a way to uniquely identify and manage separate conversations 
or threads within a system that handles multiple interactions. By assigning a unique `thread_id` to each 
conversation, the system can track the conversation context and memory more efficiently. This ensures that 
the responses generated by the model are relevant to the ongoing conversation, preventing confusion between 
different sessions. In practical terms, when multiple users or a single user engages in multiple parallel 
interactions, the `thread_id` ensures that each conversation maintains its own state, history, and context. 
It also allows for thread-specific configurations, such as adjusting conversation parameters, handling memory 
separately, and tailoring the system's behavior based on the ongoing conversation. Essentially, it helps keep 
conversations organized and ensures a seamless experience, even when managing numerous concurrent threads.
"""

while True: 
    user_input = input("User: ")
    if(user_input in ["exit", "end"]):
        break
    else: 
        result = app.invoke({
            "messages": [HumanMessage(content=user_input)]
        }, config=config)

        print("AI: " + result["messages"][-1].content)

"""
How the State is Saved:
The BasicChatState (with its messages list) is passed into the MemorySaver object.

The MemorySaver stores the entire BasicChatState object at each point in time. 
As the messages list grows with new messages (both human and AI), the state (which includes the messages list)
is updated and saved by the MemorySaver.


"""